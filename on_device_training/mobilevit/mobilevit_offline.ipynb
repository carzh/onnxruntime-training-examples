{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileViT Offline Processing\n",
    "\n",
    "On-device training requires steps that happen off the device, referred to as \"offline\" steps. \n",
    "\n",
    "This notebook contains the offline processing steps for an example using MobileViT for facial expression recognition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training artifact generation\n",
    "\n",
    "In order to train on the device, the following files are required: a model checkpoint with the weights, the training model ONNX file, the optimizer ONNX file, and the evaluation model ONNX file. \n",
    "\n",
    "The generate_artifacts method simplifies this process, allowing you to pass in an initial ONNX model (for example, imported from HuggingFace Transformers), specify a loss type and optimizer (both required), and will generate the required training artifacts for you.\n",
    "\n",
    "The parameters that require gradient and the frozen parameters are taken from the initial PyTorch model.\n",
    "\n",
    "Before passing to the generate_artifacts function, the model is configured to suit the dataset: for example, the random input that the model is built off of has the same image dimensions as the dataset images, and the number of labels is configured to reflect the dataset. \n",
    "\n",
    "Although this example uses PyTorch to export a HuggingFace Transformers model into an ONNX file to be passed to generate_artifacts, any method of creating or exporting an ONNX file can be used with generate_artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import onnx\n",
    "from onnxruntime.training import artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the configuration to reflect the number of labels used in the dataset\n",
    "config = transformers.MobileViTConfig.from_pretrained(\"apple/mobilevit-xx-small\", num_labels=7)\n",
    "model = transformers.MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-xx-small\", config=config, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_name = \"mobilevit_init_test.onnx\"\n",
    "\n",
    "# generates random pixel values for 5 images\n",
    "# originally images were 256 by 256\n",
    "random_input = {\"pixel_values\": torch.rand(5, 3, 512, 512),\n",
    "                \"labels\": torch.randint(0, 6, (5,))}\n",
    "# random_input = torch.rand(5, 3, 256, 256)\n",
    "\n",
    "torch.onnx.export(model, random_input, onnx_name,\n",
    "                    input_names=[\"pixel_values\", \"labels\"], output_names=[\"output\"],\n",
    "                    export_params=True,\n",
    "                    dynamic_axes={\n",
    "                        \"pixel_values\": {0: \"batch_size\"},\n",
    "                        \"labels\": {0: \"batch_size\"},\n",
    "                        \"output\": {0: \"batch_size\"}\n",
    "                    },\n",
    "                    do_constant_folding=False,\n",
    "                    training=torch.onnx.TrainingMode.TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requires_grad = []\n",
    "frozen_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        requires_grad.append(name)\n",
    "    else:\n",
    "        frozen_params.append(name)\n",
    "\n",
    "for name, param in model.named_buffers():\n",
    "    frozen_params.append(name)\n",
    "\n",
    "onnx_model = onnx.load(onnx_name)\n",
    "\n",
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    loss=artifacts.LossType.CrossEntropyLoss,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
