{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "The MobileViT model takes in two tensors, in the following shape:\n",
    "```\n",
    "input = {\n",
    "    \"pixel_values\": torch.FloatTensor of shape (batch_size, num_channels, height, width),\n",
    "    \"labels\": torch.LongTensor of shape (batch_size,)\n",
    "}\n",
    "```\n",
    "\n",
    "where \n",
    "* batch_size = number of images in each batch\n",
    "* num_channels = the number of color channels (i.e., RGB = 3 channels, RGBA = 4 channels)\n",
    "* height = the height of the image in pixels\n",
    "* width = the width of the image in pixels\n",
    "\n",
    "For convenience, processing a dataset of images and labels into this shape will be done in this Jupyter Notebook so that the C# app can simply read in the input in the required format. However, since this data processing only requires translating images into numbers, this step can be done in any language, and to any file format (JSON was chosen for convenience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (C:/Users/carolinezhu/.cache/huggingface/datasets/FER-Universe___imagefolder/FER-Universe--DiffusionFER-584755ddb54e5e9f/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fab23a228ff4f5f9b8bef27010f8cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"FER-Universe/DiffusionFER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([2, 3, 256, 256])\n",
      "torch.Size([3, 3, 256, 256])\n",
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([5, 3, 256, 256])\n",
      "torch.Size([6, 3, 256, 256])\n",
      "torch.Size([7, 3, 256, 256])\n",
      "torch.Size([8, 3, 256, 256])\n",
      "torch.Size([9, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([11, 3, 256, 256])\n",
      "torch.Size([12, 3, 256, 256])\n",
      "torch.Size([13, 3, 256, 256])\n",
      "torch.Size([14, 3, 256, 256])\n",
      "torch.Size([15, 3, 256, 256])\n",
      "torch.Size([16, 3, 256, 256])\n",
      "torch.Size([17, 3, 256, 256])\n",
      "torch.Size([18, 3, 256, 256])\n",
      "torch.Size([19, 3, 256, 256])\n",
      "torch.Size([20, 3, 256, 256])\n",
      "torch.Size([21, 3, 256, 256])\n",
      "torch.Size([22, 3, 256, 256])\n",
      "torch.Size([23, 3, 256, 256])\n",
      "torch.Size([24, 3, 256, 256])\n",
      "torch.Size([25, 3, 256, 256])\n",
      "torch.Size([26, 3, 256, 256])\n",
      "torch.Size([27, 3, 256, 256])\n",
      "torch.Size([28, 3, 256, 256])\n",
      "torch.Size([29, 3, 256, 256])\n",
      "torch.Size([30, 3, 256, 256])\n",
      "torch.Size([31, 3, 256, 256])\n",
      "torch.Size([32, 3, 256, 256])\n",
      "torch.Size([33, 3, 256, 256])\n",
      "torch.Size([34, 3, 256, 256])\n",
      "torch.Size([35, 3, 256, 256])\n",
      "torch.Size([36, 3, 256, 256])\n",
      "torch.Size([37, 3, 256, 256])\n",
      "torch.Size([38, 3, 256, 256])\n",
      "torch.Size([39, 3, 256, 256])\n",
      "torch.Size([40, 3, 256, 256])\n",
      "torch.Size([41, 3, 256, 256])\n",
      "torch.Size([42, 3, 256, 256])\n",
      "torch.Size([43, 3, 256, 256])\n",
      "torch.Size([44, 3, 256, 256])\n",
      "torch.Size([45, 3, 256, 256])\n",
      "torch.Size([46, 3, 256, 256])\n",
      "torch.Size([47, 3, 256, 256])\n",
      "torch.Size([48, 3, 256, 256])\n",
      "torch.Size([49, 3, 256, 256])\n",
      "torch.Size([50, 3, 256, 256])\n",
      "torch.Size([51, 3, 256, 256])\n",
      "torch.Size([52, 3, 256, 256])\n",
      "torch.Size([53, 3, 256, 256])\n",
      "torch.Size([54, 3, 256, 256])\n",
      "torch.Size([55, 3, 256, 256])\n",
      "torch.Size([56, 3, 256, 256])\n",
      "torch.Size([57, 3, 256, 256])\n",
      "torch.Size([58, 3, 256, 256])\n",
      "torch.Size([59, 3, 256, 256])\n",
      "torch.Size([60, 3, 256, 256])\n",
      "torch.Size([61, 3, 256, 256])\n",
      "torch.Size([62, 3, 256, 256])\n",
      "torch.Size([63, 3, 256, 256])\n",
      "torch.Size([64, 3, 256, 256])\n",
      "torch.Size([65, 3, 256, 256])\n",
      "torch.Size([66, 3, 256, 256])\n",
      "torch.Size([67, 3, 256, 256])\n",
      "torch.Size([68, 3, 256, 256])\n",
      "torch.Size([69, 3, 256, 256])\n",
      "torch.Size([70, 3, 256, 256])\n",
      "torch.Size([71, 3, 256, 256])\n",
      "torch.Size([72, 3, 256, 256])\n",
      "torch.Size([73, 3, 256, 256])\n",
      "torch.Size([74, 3, 256, 256])\n",
      "torch.Size([75, 3, 256, 256])\n",
      "torch.Size([76, 3, 256, 256])\n",
      "torch.Size([77, 3, 256, 256])\n",
      "torch.Size([78, 3, 256, 256])\n",
      "torch.Size([79, 3, 256, 256])\n",
      "torch.Size([80, 3, 256, 256])\n",
      "torch.Size([81, 3, 256, 256])\n",
      "torch.Size([82, 3, 256, 256])\n",
      "torch.Size([83, 3, 256, 256])\n",
      "torch.Size([84, 3, 256, 256])\n",
      "torch.Size([85, 3, 256, 256])\n",
      "torch.Size([86, 3, 256, 256])\n",
      "torch.Size([87, 3, 256, 256])\n",
      "torch.Size([88, 3, 256, 256])\n",
      "torch.Size([89, 3, 256, 256])\n",
      "torch.Size([90, 3, 256, 256])\n",
      "torch.Size([91, 3, 256, 256])\n",
      "torch.Size([92, 3, 256, 256])\n",
      "torch.Size([93, 3, 256, 256])\n",
      "torch.Size([94, 3, 256, 256])\n",
      "torch.Size([95, 3, 256, 256])\n",
      "torch.Size([96, 3, 256, 256])\n",
      "torch.Size([97, 3, 256, 256])\n",
      "torch.Size([98, 3, 256, 256])\n",
      "torch.Size([99, 3, 256, 256])\n",
      "torch.Size([100, 3, 256, 256])\n",
      "torch.Size([101, 3, 256, 256])\n",
      "torch.Size([102, 3, 256, 256])\n",
      "torch.Size([103, 3, 256, 256])\n",
      "torch.Size([104, 3, 256, 256])\n",
      "torch.Size([105, 3, 256, 256])\n",
      "torch.Size([106, 3, 256, 256])\n",
      "torch.Size([107, 3, 256, 256])\n",
      "torch.Size([108, 3, 256, 256])\n",
      "torch.Size([109, 3, 256, 256])\n",
      "torch.Size([110, 3, 256, 256])\n",
      "torch.Size([111, 3, 256, 256])\n",
      "torch.Size([112, 3, 256, 256])\n",
      "torch.Size([113, 3, 256, 256])\n",
      "torch.Size([114, 3, 256, 256])\n",
      "torch.Size([115, 3, 256, 256])\n",
      "torch.Size([116, 3, 256, 256])\n",
      "torch.Size([117, 3, 256, 256])\n",
      "torch.Size([118, 3, 256, 256])\n",
      "torch.Size([119, 3, 256, 256])\n",
      "torch.Size([120, 3, 256, 256])\n",
      "torch.Size([121, 3, 256, 256])\n",
      "torch.Size([122, 3, 256, 256])\n",
      "torch.Size([123, 3, 256, 256])\n",
      "torch.Size([124, 3, 256, 256])\n",
      "torch.Size([125, 3, 256, 256])\n",
      "torch.Size([126, 3, 256, 256])\n",
      "torch.Size([127, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([129, 3, 256, 256])\n",
      "torch.Size([130, 3, 256, 256])\n",
      "torch.Size([131, 3, 256, 256])\n",
      "torch.Size([132, 3, 256, 256])\n",
      "torch.Size([133, 3, 256, 256])\n",
      "torch.Size([134, 3, 256, 256])\n",
      "torch.Size([135, 3, 256, 256])\n",
      "torch.Size([136, 3, 256, 256])\n",
      "torch.Size([137, 3, 256, 256])\n",
      "torch.Size([138, 3, 256, 256])\n",
      "torch.Size([139, 3, 256, 256])\n",
      "torch.Size([140, 3, 256, 256])\n",
      "torch.Size([141, 3, 256, 256])\n",
      "torch.Size([142, 3, 256, 256])\n",
      "torch.Size([143, 3, 256, 256])\n",
      "torch.Size([144, 3, 256, 256])\n",
      "torch.Size([145, 3, 256, 256])\n",
      "torch.Size([146, 3, 256, 256])\n",
      "torch.Size([147, 3, 256, 256])\n",
      "torch.Size([148, 3, 256, 256])\n",
      "torch.Size([149, 3, 256, 256])\n",
      "torch.Size([150, 3, 256, 256])\n",
      "torch.Size([151, 3, 256, 256])\n",
      "torch.Size([152, 3, 256, 256])\n",
      "torch.Size([153, 3, 256, 256])\n",
      "torch.Size([154, 3, 256, 256])\n",
      "torch.Size([155, 3, 256, 256])\n",
      "torch.Size([156, 3, 256, 256])\n",
      "torch.Size([157, 3, 256, 256])\n",
      "torch.Size([158, 3, 256, 256])\n",
      "torch.Size([159, 3, 256, 256])\n",
      "torch.Size([160, 3, 256, 256])\n",
      "torch.Size([161, 3, 256, 256])\n",
      "torch.Size([162, 3, 256, 256])\n",
      "torch.Size([163, 3, 256, 256])\n",
      "torch.Size([164, 3, 256, 256])\n",
      "torch.Size([165, 3, 256, 256])\n",
      "torch.Size([166, 3, 256, 256])\n",
      "torch.Size([167, 3, 256, 256])\n",
      "torch.Size([168, 3, 256, 256])\n",
      "torch.Size([169, 3, 256, 256])\n",
      "torch.Size([170, 3, 256, 256])\n",
      "torch.Size([171, 3, 256, 256])\n",
      "torch.Size([172, 3, 256, 256])\n",
      "torch.Size([173, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "tensor_converter = transforms.Compose([\n",
    "    # dataset has varying sizes of images; resizing to a power of 2 to match ONNX model inputs\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def convert_to_tensor(list_of_png, labels):\n",
    "    # return [tensor_converter(image) for image in list_of_png]\n",
    "    mega_tensor = None\n",
    "    count = 0\n",
    "    new_labels = []\n",
    "    # for the sake of the demo, skip through some of the examples for a smaller dataset\n",
    "    for i in range(0, len(list_of_png), 15):\n",
    "    # for i in range(3):\n",
    "        png = list_of_png[i]\n",
    "        if mega_tensor is None:\n",
    "            mega_tensor = tensor_converter(png).unsqueeze(0)\n",
    "        else:\n",
    "            mega_tensor = torch.vstack((mega_tensor, tensor_converter(png).unsqueeze(0)))\n",
    "\n",
    "        new_labels.append(labels[i])\n",
    "        print(mega_tensor.shape)\n",
    "    return mega_tensor, new_labels\n",
    "\n",
    "images, labels = convert_to_tensor(dataset['train']['image'], dataset['train']['label'])\n",
    "\n",
    "tensor_dataset = {\n",
    "    'image': images,\n",
    "    'label': labels\n",
    "    # concatenate the labels to be the same length if using for each loop when processing the images\n",
    "    # 'label': dataset['train']['label'][:images.shape[0]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_dict(tensor_dict, keys_tensors, keys_1d):\n",
    "    \"\"\"\n",
    "    Takes in a dictionary where the values are tensors\n",
    "\n",
    "    Basically changes the 2d Python lists into two fields: a shape & a flattened list, for easier conversion to OnnxValues\n",
    "\n",
    "    Returns a dictionary\n",
    "    \"\"\"\n",
    "    json_dict = {}\n",
    "\n",
    "    for key_name in keys_tensors:\n",
    "        # add field for the shape of the tensor\n",
    "        json_dict[key_name + \"_shape\"] = list(tensor_dict[key_name].shape)\n",
    "        # flatten list\n",
    "        json_dict[key_name] = torch.flatten(tensor_dict[key_name]).tolist()\n",
    "\n",
    "    for key_name in keys_1d:\n",
    "        # add field for the shape of the tensor\n",
    "        json_dict[key_name + \"_shape\"] = [len(tensor_dict[key_name])]\n",
    "        json_dict[key_name] = tensor_dict[key_name]\n",
    "    \n",
    "    \n",
    "    return json_dict\n",
    "\n",
    "json_dict = generate_json_dict(tensor_dataset, ['image'], ['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([173, 3, 256, 256])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_dataset['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('mini_train.json', 'w') as json_file:\n",
    "    json.dump(json_dict, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2edemos-kernel",
   "language": "python",
   "name": "e2edemos-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
